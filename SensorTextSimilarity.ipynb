{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1f57d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training (Cosine Similarity Contrastive Loss) ---\n",
      "Total training pairs: 8400\n",
      "Epoch 5/30, Average Loss: 0.2513\n",
      "Epoch 10/30, Average Loss: 0.2503\n",
      "Epoch 15/30, Average Loss: 0.2496\n",
      "Epoch 20/30, Average Loss: 0.2486\n",
      "Epoch 25/30, Average Loss: 0.2491\n",
      "Epoch 30/30, Average Loss: 0.2484\n",
      "\n",
      "Training complete.\n",
      "Models saved: sensor_encoder_wisdom_3col.pth and text_encoder_wisdom_3col.pth\n",
      "\n",
      "--- Starting Evaluation on Test Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siu856558563\\AppData\\Local\\Temp\\ipykernel_14304\\3215355506.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sensor_encoder.load_state_dict(torch.load(SENSOR_MODEL_PATH))\n",
      "C:\\Users\\siu856558563\\AppData\\Local\\Temp\\ipykernel_14304\\3215355506.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_encoder.load_state_dict(torch.load(TEXT_MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 3600\n",
      "Mean Similarity (Positive Pairs): 0.5715\n",
      "Mean Similarity (Negative Pairs): 0.5805\n",
      "Matching Accuracy (Similarity > MARGIN=0.5): 50.28%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "EMBEDDING_DIM = 64     # The size of the shared embedding space\n",
    "MARGIN = 0.5           # Similarity Margin: Pairs with Similarity > 0.5 are considered a match\n",
    "EPOCHS = 30            # Training epochs\n",
    "BATCH_SIZE = 4         # Batch size (small due to the small dataset)\n",
    "LEARNING_RATE = 1e-4   # Learning rate\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\" # Pre-trained language model\n",
    "\n",
    "# --- UPDATED CONFIGURATION FOR WISDOM DATA ---\n",
    "SENSOR_FEATURES_COUNT = 3 # **UPDATED: Only X_Acc, Y_Acc, Z_Acc are present**\n",
    "TEXT_COLUMN_NAME = 'Semantic_Interpretation'\n",
    "SENSOR_COLUMNS = ['X_Acc', 'Y_Acc', 'Z_Acc']\n",
    "SENSOR_MODEL_PATH = 'sensor_encoder_wisdom_3col.pth'\n",
    "TEXT_MODEL_PATH = 'text_encoder_wisdom_3col.pth'\n",
    "DATA_FILE = \"./data/WISDM_with_semantic_interpretation.csv\" # **UPDATED data file**\n",
    "\n",
    "# --- 2. DATASET CLASS ---\n",
    "\n",
    "class SensorTextDataset(Dataset):\n",
    "    def __init__(self, sensor_data, text_data, labels):\n",
    "        # Convert sensor data to tensor\n",
    "        self.sensor_data = torch.tensor(sensor_data, dtype=torch.float32)\n",
    "        self.text_data = text_data\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sensor_data[idx], self.text_data[idx], self.labels[idx]\n",
    "\n",
    "# --- 3. DUAL-ENCODER ARCHITECTURE ---\n",
    "\n",
    "# Encoder for Sensor Features (Simple Multi-Layer Perceptron)\n",
    "class SensorEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Encoder for Text Descriptions (Pre-trained DistilBERT)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, output_dim):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # Projection layer aligns BERT output (e.g., 768 dim) to the shared embedding space (64 dim)\n",
    "        self.projection = nn.Linear(self.model.config.hidden_size, output_dim)\n",
    "    def forward(self, texts):\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        output = self.model(**encoded_input)\n",
    "        mean_pooled = torch.mean(output.last_hidden_state, dim=1) # Mean-pooling for sentence embedding\n",
    "        return self.projection(mean_pooled)\n",
    "\n",
    "# --- 4. CONTRASTIVE LOSS FUNCTION (Cosine Similarity) ---\n",
    "\n",
    "class ContrastiveSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive Loss based on Cosine Similarity (S).\n",
    "    L = y * (1 - S) + (1 - y) * max(0, S - margin)\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(ContrastiveSimilarityLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Calculate Cosine Similarity\n",
    "        similarity = F.cosine_similarity(output1, output2).unsqueeze(1)\n",
    "        \n",
    "        # Prepare tensors for loss calculation\n",
    "        device = output1.device\n",
    "        zero_tensor = torch.tensor(0.0).to(device)\n",
    "        margin_tensor = torch.tensor(self.margin).to(device)\n",
    "\n",
    "        # Positive pairs (label == 1.0): Loss is 1 - S (Minimized when S is high)\n",
    "        loss_positive = label * (1 - similarity)\n",
    "        \n",
    "        # Negative pairs (label == 0.0): Loss is max(0, S - margin) (Minimized when S is low)\n",
    "        loss_negative = (1 - label) * torch.max(zero_tensor, similarity - margin_tensor)\n",
    "        \n",
    "        loss = torch.mean(loss_positive + loss_negative)\n",
    "        return loss\n",
    "\n",
    "# --- 5. TRAINING FUNCTION ---\n",
    "\n",
    "def train_contrastive_model():\n",
    "    print(\"--- Starting Training (Cosine Similarity Contrastive Loss) ---\")\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    \n",
    "    # --- UPDATED SENSOR COLUMN SELECTION LOGIC ---\n",
    "    # Use the explicitly defined sensor columns (X_Acc, Y_Acc, Z_Acc)\n",
    "    sensor_cols = SENSOR_COLUMNS \n",
    "\n",
    "    # Safety check\n",
    "    if set(sensor_cols).issubset(df.columns) is False:\n",
    "        raise ValueError(f\"Required sensor columns {sensor_cols} not found in {DATA_FILE}\")\n",
    "\n",
    "    if len(sensor_cols) != SENSOR_FEATURES_COUNT:\n",
    "        raise ValueError(f\"Feature count mismatch! Expected {SENSOR_FEATURES_COUNT} features, but detected {len(sensor_cols)} in the data file: {DATA_FILE}\")\n",
    "\n",
    "    sensor_data = df[sensor_cols].values\n",
    "    text_data = df[TEXT_COLUMN_NAME].tolist()\n",
    "\n",
    "    # Create POSITIVE and NEGATIVE training samples (by rolling/mismatching the text)\n",
    "    pos_sensor, pos_text, pos_labels = sensor_data, text_data, np.ones(len(df))\n",
    "    \n",
    "    # Simple rolling for negative samples\n",
    "    neg_sensor = sensor_data\n",
    "    neg_text = np.roll(text_data, 1).tolist()\n",
    "    neg_labels = np.zeros(len(df))\n",
    "\n",
    "    all_sensor = np.concatenate([pos_sensor, neg_sensor])\n",
    "    all_text = pos_text + neg_text\n",
    "    all_labels = np.concatenate([pos_labels, neg_labels])\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train_s, X_test_s, X_train_t, X_test_t, y_train, y_test = train_test_split(\n",
    "        all_sensor, all_text, all_labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    sensor_encoder = SensorEncoder(SENSOR_FEATURES_COUNT, EMBEDDING_DIM)\n",
    "    text_encoder = TextEncoder(TEXT_MODEL_NAME, EMBEDDING_DIM)\n",
    "    criterion = ContrastiveSimilarityLoss(margin=MARGIN)\n",
    "    \n",
    "    params = list(sensor_encoder.parameters()) + list(text_encoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=LEARNING_RATE)\n",
    "\n",
    "    train_dataset = SensorTextDataset(X_train_s, X_train_t, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Total training pairs: {len(train_dataset)}\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for sensor_batch, text_batch, label_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            sensor_embedding = sensor_encoder(sensor_batch)\n",
    "            text_embedding = text_encoder(text_batch)\n",
    "            loss = criterion(sensor_embedding, text_embedding, label_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    \n",
    "    # Save the trained encoders\n",
    "    torch.save(sensor_encoder.state_dict(), SENSOR_MODEL_PATH)\n",
    "    torch.save(text_encoder.state_dict(), TEXT_MODEL_PATH)\n",
    "    print(f\"Models saved: {SENSOR_MODEL_PATH} and {TEXT_MODEL_PATH}\")\n",
    "\n",
    "    return X_test_s, X_test_t, y_test\n",
    "\n",
    "# --- 6. EVALUATION FUNCTION ---\n",
    "\n",
    "def evaluate_contrastive_model(X_test_s, X_test_t, y_test):\n",
    "    print(\"\\n--- Starting Evaluation on Test Data ---\")\n",
    "    \n",
    "    sensor_encoder = SensorEncoder(SENSOR_FEATURES_COUNT, EMBEDDING_DIM)\n",
    "    text_encoder = TextEncoder(TEXT_MODEL_NAME, EMBEDDING_DIM)\n",
    "\n",
    "    # Note: These lines will only work if the user has saved these files locally after training\n",
    "    sensor_encoder.load_state_dict(torch.load(SENSOR_MODEL_PATH))\n",
    "    text_encoder.load_state_dict(torch.load(TEXT_MODEL_PATH))\n",
    "\n",
    "    sensor_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    test_dataset = SensorTextDataset(X_test_s, X_test_t, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    similarities = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sensor_batch, text_batch, label_batch in test_loader:\n",
    "            sensor_embedding = sensor_encoder(sensor_batch)\n",
    "            text_embedding = text_encoder(text_batch)\n",
    "            sim = F.cosine_similarity(sensor_embedding, text_embedding)\n",
    "            similarities.extend(sim.cpu().numpy())\n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "    similarities = np.array(similarities)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    pos_similarities = similarities[true_labels == 1.0]\n",
    "    neg_similarities = similarities[true_labels == 0.0]\n",
    "\n",
    "    print(f\"Test Set Size: {len(true_labels)}\")\n",
    "    print(f\"Mean Similarity (Positive Pairs): {np.mean(pos_similarities):.4f}\")\n",
    "    print(f\"Mean Similarity (Negative Pairs): {np.mean(neg_similarities):.4f}\")\n",
    "    \n",
    "    # Prediction: A pair is POSITIVE if its similarity is greater than the MARGIN\n",
    "    predictions = (similarities > MARGIN).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Matching Accuracy (Similarity > MARGIN={MARGIN}): {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The script runs training and then evaluates the model on the held-out test data\n",
    "    try:\n",
    "        X_test_s, X_test_t, y_test = train_contrastive_model()\n",
    "        evaluate_contrastive_model(X_test_s, X_test_t, y_test)\n",
    "    except Exception as e:\n",
    "        print(\"\\n----------------------------------------------------------------------\")\n",
    "        print(\"ðŸ›‘ **EXECUTION REMINDER** ðŸ›‘\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"\\n**ACTION REQUIRED:** This code is for sensor-text alignment. Please copy the entire code block above and run it in your local Python environment where **PyTorch, Transformers, and scikit-learn** are installed.\")\n",
    "        print(\"----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3-vision-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
