{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b5074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siu856558563\\AppData\\Local\\anaconda3\\envs\\contrastive_model_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading dataset ===\n",
      "Using text column: time\n",
      "Using 14 sensor columns: ['chest_acc_x', 'chest_acc_y', 'chest_acc_z', 'chest_ecg_ch1', 'chest_emg_ch1', 'chest_eda_ch1'] ...\n",
      "Training samples: 12800  |  Sensor dim: 14  |  Device: cpu\n",
      "Epoch 01/15 - loss: 0.2519\n",
      "Epoch 02/15 - loss: 0.2497\n",
      "Epoch 04/15 - loss: 0.2488\n",
      "Epoch 06/15 - loss: 0.2480\n",
      "Epoch 08/15 - loss: 0.2474\n",
      "Epoch 10/15 - loss: 0.2477\n",
      "Epoch 12/15 - loss: 0.2471\n",
      "Epoch 14/15 - loss: 0.2468\n",
      "Epoch 15/15 - loss: 0.2468\n",
      "Saved: sensor_encoder_wesad.pth, text_encoder_wesad.pth\n",
      "\n",
      "=== Evaluating on held-out test set ===\n",
      "Test size: 3200\n",
      "Mean similarity (pos): 0.7748 | (neg): 0.8129\n",
      "Matching accuracy (threshold=0.5): 48.72%\n"
     ]
    }
   ],
   "source": [
    "# train_wesad_alignment.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===============================\n",
    "# 1) CONFIGURATION\n",
    "# ===============================\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EMBEDDING_DIM = 64            # shared embedding size\n",
    "MARGIN = 0.5                  # similarity threshold used for \"match\" decision\n",
    "EPOCHS = 15                   # WESAD rows can be many; start modest\n",
    "BATCH_SIZE = 16               # larger batch is fine with GPU; adjust if OOM\n",
    "LEARNING_RATE = 1e-4\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# === Attached dataset path ===\n",
    "DATA_FILE = \"C:/Users/siu856558563/OneDrive - Southern Illinois University/Documents/Attack/SensorEncoderAttack/Data/WESAD.csv\"\n",
    "\n",
    "# Columns (the code will auto-detect if these are absent)\n",
    "PREFERRED_TEXT_COLS = [\"Semantic_Interpretation\", \"text\", \"Text\", \"semantic\", \"caption\"]\n",
    "EXCLUDE_COLS = {\"Label\", \"label\"}  # exclude from sensor features if present\n",
    "\n",
    "# Model save paths\n",
    "SENSOR_MODEL_PATH = \"sensor_encoder_wesad.pth\"\n",
    "TEXT_MODEL_PATH = \"text_encoder_wesad.pth\"\n",
    "\n",
    "# Optional tokenizer max length\n",
    "MAX_LEN = 128\n",
    "\n",
    "# ===============================\n",
    "# 2) DATASET\n",
    "# ===============================\n",
    "class SensorTextDataset(Dataset):\n",
    "    def __init__(self, sensor: np.ndarray, texts: List[str], labels: np.ndarray):\n",
    "        self.sensor = torch.tensor(sensor, dtype=torch.float32)\n",
    "        self.texts = list(texts)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return texts raw; tokenization is done inside the text encoder\n",
    "        return self.sensor[idx], self.texts[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Keep text as a list for tokenizer; stack sensors/labels into tensors.\n",
    "    \"\"\"\n",
    "    sensors = torch.stack([b[0] for b in batch], dim=0)\n",
    "    texts = [b[1] for b in batch]\n",
    "    labels = torch.stack([b[2] for b in batch], dim=0)\n",
    "    return sensors, texts, labels\n",
    "\n",
    "# ===============================\n",
    "# 3) MODELS\n",
    "# ===============================\n",
    "class SensorEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, texts: List[str]):\n",
    "        # Tokenize on the fly; move to the same device as the BERT model\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.to(self.bert.device) for k, v in enc.items()}\n",
    "        out = self.bert(**enc).last_hidden_state  # [B, T, H]\n",
    "        pooled = out.mean(dim=1)                  # mean pool -> [B, H]\n",
    "        return self.proj(pooled)\n",
    "\n",
    "# ===============================\n",
    "# 4) LOSS\n",
    "# ===============================\n",
    "class ContrastiveSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    L = y * (1 - S) + (1 - y) * max(0, S - margin), where S = cosine_similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, u, v, y):\n",
    "        # u, v: [B, D]; y: [B] or [B, 1]\n",
    "        sim = F.cosine_similarity(u, v)  # [B]\n",
    "        if y.dim() > 1:\n",
    "            y = y.squeeze(1)\n",
    "        zero = torch.zeros_like(sim)\n",
    "        loss_pos = y * (1.0 - sim)                 # push positives to high similarity\n",
    "        loss_neg = (1.0 - y) * torch.clamp(sim - self.margin, min=0.0)\n",
    "        return (loss_pos + loss_neg).mean()\n",
    "\n",
    "# ===============================\n",
    "# 5) UTILS\n",
    "# ===============================\n",
    "def find_text_column(df: pd.DataFrame) -> str:\n",
    "    # Prefer known names\n",
    "    for c in PREFERRED_TEXT_COLS:\n",
    "        if c in df.columns and df[c].dtype == object:\n",
    "            return c\n",
    "    # Otherwise pick the first object dtype column with non-empty strings\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if len(obj_cols) == 0:\n",
    "        # If nothing object-typed, try to coerce any column to string that looks textual\n",
    "        raise ValueError(\n",
    "            \"No textual (object) column found for semantic descriptions. \"\n",
    "            f\"Tried: {PREFERRED_TEXT_COLS}. Please add one to {DATA_FILE}.\"\n",
    "        )\n",
    "    return obj_cols[0]\n",
    "\n",
    "def select_sensor_columns(df: pd.DataFrame, text_col: str) -> List[str]:\n",
    "    # numeric columns except excluded and the text column\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    candidates = [c for c in num_cols if c not in EXCLUDE_COLS and c != text_col]\n",
    "    if len(candidates) == 0:\n",
    "        raise ValueError(\"No numeric sensor feature columns detected.\")\n",
    "    return candidates\n",
    "\n",
    "def standardize_train_test(train: np.ndarray, test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # simple standardization (z-score) per feature using train stats\n",
    "    mean = train.mean(axis=0, keepdims=True)\n",
    "    std = train.std(axis=0, keepdims=True) + 1e-8\n",
    "    return (train - mean) / std, (test - mean) / std\n",
    "\n",
    "# ===============================\n",
    "# 6) TRAIN\n",
    "# ===============================\n",
    "def train():\n",
    "    print(\"=== Loading dataset ===\")\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        raise FileNotFoundError(f\"Data file not found: {DATA_FILE}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    text_col = find_text_column(df)\n",
    "    sensor_cols = select_sensor_columns(df, text_col)\n",
    "\n",
    "    print(f\"Using text column: {text_col}\")\n",
    "    print(f\"Using {len(sensor_cols)} sensor columns: {sensor_cols[:6]}{' ...' if len(sensor_cols)>6 else ''}\")\n",
    "\n",
    "    # Pull raw arrays\n",
    "    sensors = df[sensor_cols].to_numpy(dtype=np.float32)\n",
    "    texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "    # Build synthetic positives (aligned) and negatives (misaligned via one-step roll)\n",
    "    pos_sensor, pos_text, pos_labels = sensors, texts, np.ones(len(df), dtype=np.float32)\n",
    "    neg_sensor, neg_text, neg_labels = sensors, np.roll(texts, 1).tolist(), np.zeros(len(df), dtype=np.float32)\n",
    "\n",
    "    all_sensor = np.concatenate([pos_sensor, neg_sensor], axis=0)\n",
    "    all_text = pos_text + neg_text\n",
    "    all_labels = np.concatenate([pos_labels, neg_labels], axis=0)\n",
    "\n",
    "    # Train/test split\n",
    "    Xs_tr, Xs_te, Xt_tr, Xt_te, y_tr, y_te = train_test_split(\n",
    "        all_sensor, all_text, all_labels, test_size=0.2, random_state=SEED, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Standardize numeric features\n",
    "    Xs_tr, Xs_te = standardize_train_test(Xs_tr, Xs_te)\n",
    "\n",
    "    # Build models\n",
    "    sensor_encoder = SensorEncoder(input_dim=all_sensor.shape[1], output_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "    text_encoder = TextEncoder(TEXT_MODEL_NAME, EMBEDDING_DIM).to(DEVICE)\n",
    "    criterion = ContrastiveSimilarityLoss(margin=MARGIN)\n",
    "    optimizer = optim.Adam(list(sensor_encoder.parameters()) + list(text_encoder.parameters()),\n",
    "                           lr=LEARNING_RATE)\n",
    "\n",
    "    train_ds = SensorTextDataset(Xs_tr, Xt_tr, y_tr)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Training samples: {len(train_ds)}  |  Sensor dim: {all_sensor.shape[1]}  |  Device: {DEVICE}\")\n",
    "\n",
    "    sensor_encoder.train()\n",
    "    text_encoder.train()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        running = 0.0\n",
    "        for sensor_batch, text_batch, label_batch in train_loader:\n",
    "            sensor_batch = sensor_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            z_s = sensor_encoder(sensor_batch)\n",
    "            z_t = text_encoder(text_batch)  # text encoder handles its own device\n",
    "\n",
    "            loss = criterion(z_s, z_t, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        avg = running / max(1, len(train_loader))\n",
    "        if epoch == 1 or epoch % 2 == 0 or epoch == EPOCHS:\n",
    "            print(f\"Epoch {epoch:02d}/{EPOCHS} - loss: {avg:.4f}\")\n",
    "\n",
    "    # Save models\n",
    "    torch.save(sensor_encoder.state_dict(), SENSOR_MODEL_PATH)\n",
    "    torch.save(text_encoder.state_dict(), TEXT_MODEL_PATH)\n",
    "    print(f\"Saved: {SENSOR_MODEL_PATH}, {TEXT_MODEL_PATH}\")\n",
    "\n",
    "    return Xs_te, Xt_te, y_te, sensor_cols, text_col\n",
    "\n",
    "# ===============================\n",
    "# 7) EVALUATE\n",
    "# ===============================\n",
    "def evaluate(Xs_te, Xt_te, y_te, sensor_cols):\n",
    "    print(\"\\n=== Evaluating on held-out test set ===\")\n",
    "    sensor_encoder = SensorEncoder(input_dim=len(sensor_cols), output_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "    text_encoder = TextEncoder(TEXT_MODEL_NAME, EMBEDDING_DIM).to(DEVICE)\n",
    "\n",
    "    sensor_encoder.load_state_dict(torch.load(SENSOR_MODEL_PATH, map_location=DEVICE))\n",
    "    text_encoder.load_state_dict(torch.load(TEXT_MODEL_PATH, map_location=DEVICE))\n",
    "    sensor_encoder.eval(); text_encoder.eval()\n",
    "\n",
    "    test_ds = SensorTextDataset(Xs_te, Xt_te, y_te)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    sims, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sensor_batch, text_batch, label_batch in test_loader:\n",
    "            sensor_batch = sensor_batch.to(DEVICE)\n",
    "            z_s = sensor_encoder(sensor_batch)\n",
    "            z_t = text_encoder(text_batch)\n",
    "            sim = F.cosine_similarity(z_s, z_t).detach().cpu().numpy()\n",
    "            sims.append(sim)\n",
    "            labels.append(label_batch.numpy())\n",
    "\n",
    "    sims = np.concatenate(sims)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    pos = sims[labels == 1.0]\n",
    "    neg = sims[labels == 0.0]\n",
    "    print(f\"Test size: {len(labels)}\")\n",
    "    print(f\"Mean similarity (pos): {pos.mean():.4f} | (neg): {neg.mean():.4f}\")\n",
    "\n",
    "    preds = (sims > MARGIN).astype(np.float32)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Matching accuracy (threshold={MARGIN}): {acc*100:.2f}%\")\n",
    "\n",
    "# ===============================\n",
    "# 8) MAIN\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        Xs_te, Xt_te, y_te, sensor_cols, text_col = train()\n",
    "        evaluate(Xs_te, Xt_te, y_te, sensor_cols)\n",
    "    except Exception as e:\n",
    "        print(\"\\n----------------------------------------------------------------------\")\n",
    "        print(\"ðŸ›‘ EXECUTION ERROR\")\n",
    "        print(f\"{type(e).__name__}: {e}\")\n",
    "        print(\"Tip: Ensure /mnt/data/WESAD.csv has one textual column for semantics \"\n",
    "              f\"(e.g., one of {PREFERRED_TEXT_COLS}) and the rest numeric sensor features.\")\n",
    "        print(\"----------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
