Contrastive learning has become a central technique for aligning multimodal data, particularly in sensor text systems where wearable and motion signals are paired with natural language descriptions such as daily life journaling, routine narratives, or personal assistants. This alignment enables a deeper, context-aware, adaptive, interpretable, and robust understanding of human activity recognition. However, recent findings on text and image encoders show that cross-modal alignment mechanisms can be vulnerable, raising important reliability and safety concerns. In this work, we introduce a cross-modal adversarial attack that directly targets the contrastive loss used to align sensor and text embeddings. The attack applies small, seemingly harmless perturbations to the text embeddings that manipulate the contrastive objective and induce incorrect associations with the corresponding sensor representations. Through systematic evaluation on four benchmark datasets, we observe that even minimal perturbations cause the learned representations to drift from correct activity semantics. The attack consistently lowers cosine similarity between true sensor text pairs, increases contrastive loss, degrades label consistency, and disrupts high-level semantic alignment. These effects expose a significant weakness in contrastive learning for multimodal HAR systems, one that can cause cascading errors in downstream tasks, loss of interpretability, gradual poisoning of the representation space, and reduced user trust in AI-assisted sensing. Our findings highlight the need for developing robust and secure contrastive learning frameworks to protect LLM-assisted sensor intelligence from cross-modal adversarial threats.

